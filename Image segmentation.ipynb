{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up an image segmentation\n",
    "\n",
    "In this example, we will use the tf.data package to preprocess training data and then train a UNet on a binary segmentation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tf.data.Dataset to transform inputs\n",
    "\n",
    "We'll first need some utility function to read the data. We load everything as numpy array and then\n",
    "create the tf.data pipeline from it. Note that for a large dataset, reading the data should be integrated\n",
    "into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.external.tifffile import imread\n",
    "from fnmatch import fnmatch\n",
    "from filecmp import dircmp\n",
    "\n",
    "def normalize(img):\n",
    "    '''\n",
    "    '''\n",
    "    return img.astype('float32') / 255.\n",
    "\n",
    "def load_data(folder, raw_subfolder, mask_subfolder, pattern):\n",
    "    '''loads pairs of images and annotations.\n",
    "    \n",
    "    '''\n",
    "    def _get_matching_filenames(first, second):\n",
    "        '''convenience generator to get files with matching filenames.\n",
    "        \n",
    "        '''\n",
    "        for fname in dircmp(first, second).common_files:\n",
    "            if fnmatch(fname, pattern):\n",
    "                yield os.path.join(first, fname), os.path.join(second, fname)\n",
    "                \n",
    "\n",
    "    imgs, masks = zip(*[(normalize(imread(raw_path)), imread(mask_path) >= 1) \n",
    "                        for raw_path, mask_path in _get_matching_filenames(os.path.join(folder, raw_subfolder),\n",
    "                                                                           os.path.join(folder, mask_subfolder))])\n",
    "    \n",
    "    return (np.asarray(imgs)[..., None], np.asarray(masks)[..., None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some transformations that we want to apply to the data before feeding it to the model.\n",
    "Typical transforms are taking crops in order to train on patches and data augmentations (like adding noise to the input image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(patch_size):\n",
    "    '''returns the patch sampling function that takes the same\n",
    "    random patch for all tensors in a given dictionary.\n",
    "\n",
    "    If you just want to take a random patch from a *single*\n",
    "    tensor, you should probably use tensorflow.image.random_crop\n",
    "\n",
    "    '''\n",
    "    def _cropper(*inputs):\n",
    "        '''expects a dictionary, list or tuple of tensors of\n",
    "        identical shape as inputs.\n",
    "\n",
    "        '''\n",
    "        with tf.name_scope('random_patch'):\n",
    "\n",
    "            shape = tf.shape(inputs[0])\n",
    "            size = tf.convert_to_tensor(patch_size, name='patch_size')\n",
    "            limit = shape - size + 1\n",
    "            offset = tf.random.uniform(\n",
    "                tf.shape(shape),\n",
    "                dtype=tf.int32,\n",
    "                maxval=tf.int32.max,\n",
    "            ) % limit\n",
    "\n",
    "\n",
    "            return tuple(tf.slice(value, offset, size)\n",
    "                         for value in inputs)\n",
    "\n",
    "    return _cropper\n",
    "\n",
    "\n",
    "def random_axis_flip(axis, flip_prob=0.5):\n",
    "    '''reverses axis with probability threshold for all given inputs.\n",
    "    \n",
    "    '''\n",
    "    def _flipper(*inputs):\n",
    "        '''\n",
    "        '''\n",
    "        draw_prob = tf.random.uniform(\n",
    "            shape=[], minval=0, maxval=1, dtype=tf.float32)\n",
    "\n",
    "        return tuple(tf.cond(\n",
    "                draw_prob <= flip_prob,\n",
    "                lambda: tf.reverse(val, [axis]),  # pylint: disable = W0640\n",
    "                lambda: val)                      # pylint: disable = W0640\n",
    "            for val in inputs)\n",
    "\n",
    "    return _flipper\n",
    "\n",
    "\n",
    "def gaussian_noise(noise_mu, noise_sigma, key):\n",
    "    '''adds gaussian noise to the given tensor.\n",
    "    Noise level (sigma) are sampled for each call from the given\n",
    "    noise_mu and noise_sigma.\n",
    "    '''\n",
    "\n",
    "    def _distorter(*inputs):\n",
    "        '''\n",
    "        '''\n",
    "        inputs = list(inputs)\n",
    "        sigma = tf.maximum(\n",
    "            0., tf.random.normal(shape=[], mean=noise_mu, stddev=noise_sigma))\n",
    "\n",
    "        image = inputs[key]\n",
    "        noise = tf.random.normal(\n",
    "            shape=tf.shape(image), mean=0, stddev=sigma)\n",
    "        inputs[key] = image + noise\n",
    "        return inputs\n",
    "\n",
    "    return _distorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_dataset(*args, patch_size=None, batch_size=1, patches_per_image=1, augmentations=None, **kwargs):\n",
    "    '''\n",
    "    '''\n",
    "    # some data pipeline parameters\n",
    "    shuffle_buffer = 10\n",
    "    num_parallel_calls = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    # actually create the dataset and transform it\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(load_data(*args, **kwargs))\n",
    "    \n",
    "    if patches_per_image >= 2:\n",
    "        dataset = dataset.repeat(patches_per_image)\n",
    "    \n",
    "    if patch_size is not None:\n",
    "        dataset = dataset.map(random_crop(patch_size), \n",
    "                              num_parallel_calls=num_parallel_calls)\n",
    "        \n",
    "     # apply image augmentations.\n",
    "    if augmentations is not None:\n",
    "        for augmentation_fn in augmentations:\n",
    "            dataset = dataset.map(\n",
    "                augmentation_fn, num_parallel_calls=num_parallel_calls)\n",
    "        \n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    \n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over the dataset to see if everything looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patch_size = (128, 128, 1)\n",
    "\n",
    "dataset = create_dataset('data/training/', 'raw', 'mask', '*.TIF', \n",
    "                         patch_size=patch_size, batch_size=1,\n",
    "                         augmentations=[random_axis_flip(1),\n",
    "                                        random_axis_flip(2),\n",
    "                                        gaussian_noise(0.1, 0.25, 0)])\n",
    "\n",
    "for img, annot in dataset:\n",
    "    axarr = plt.subplots(1, 2, figsize=(8, 4))[1]\n",
    "    axarr[0].imshow(img.numpy().squeeze(), cmap='Greys')\n",
    "    axarr[1].imshow(annot.numpy().squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a separate dataset for validation. This time, \n",
    "we want to yield batches of 1 without any cropping and no augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = create_dataset('data/val/', 'raw', 'mask', '*.TIF', batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_unet(n_levels, initial_features, in_channels=1, out_channels=1):\n",
    "    '''a quick and dirty implementation of a unet.\n",
    "    \n",
    "    '''\n",
    "    # unexposed parameters \n",
    "    n_blocks = 2\n",
    "    kernel_size = 3\n",
    "    pooling_size = 2\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=(None, None, in_channels), name='img')\n",
    "    x = inputs\n",
    "    \n",
    "    convparams = dict(kernel_size=kernel_size, activation='relu', padding='same')\n",
    "    \n",
    "    # downstream\n",
    "    skips = {}\n",
    "    for level in range(n_levels - 1):\n",
    "        for _ in range(n_blocks):\n",
    "            x = keras.layers.Conv2D(initial_features * 2 ** level, **convparams)(x)\n",
    "        skips[level] = x\n",
    "        x = keras.layers.MaxPool2D(pooling_size)(x)\n",
    "        \n",
    "    # lowest level\n",
    "    for _ in range(n_blocks):\n",
    "        x = keras.layers.Conv2D(initial_features * 2 ** (n_levels - 1), **convparams)(x)\n",
    "    \n",
    "    # upstream\n",
    "    for level in reversed(range(n_levels - 1)):\n",
    "        \n",
    "        # \n",
    "        x = keras.layers.Conv2DTranspose(initial_features * 2 ** level, \n",
    "                                         strides=pooling_size,\n",
    "                                         kernel_size=kernel_size, \n",
    "                                         padding=convparams['padding'], \n",
    "                                         activation=convparams['activation'])(x)\n",
    "        x = keras.layers.Concatenate()([x, skips[level]])\n",
    "        \n",
    "        for _ in range(n_blocks):\n",
    "            x = keras.layers.Conv2D(initial_features * 2 ** level, **convparams)(x)\n",
    "    \n",
    "        \n",
    "    activation = 'sigmoid' if out_channels == 1 else 'softmax'\n",
    "    x = keras.layers.Conv2D(out_channels, kernel_size=1, activation=activation, padding='same', \n",
    "                            name='mask')(x)\n",
    "    \n",
    "    return keras.Model(inputs=[inputs], outputs=[x])\n",
    "\n",
    "model = build_unet(2, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              learning_rate=0.01,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=20, \n",
    "          callbacks=[keras.callbacks.TensorBoard('unet-logs/')], \n",
    "          validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img, annotation in val_dataset:\n",
    "\n",
    "    # apply model to single training patch\n",
    "    prob = model.predict(img)\n",
    "    \n",
    "    # plotting.\n",
    "    axarr = plt.subplots(1, 3, figsize=(12, 5))[1]\n",
    "    axarr[0].imshow(img.numpy().squeeze(), cmap='Greys')\n",
    "    axarr[1].imshow(prob.squeeze())\n",
    "    axarr[2].imshow(annotation.numpy().squeeze())\n",
    "    for ax in axarr:\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also predict on an image of different size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_img = normalize(imread('data/val/raw/SIMCEPImages_A06_C23_F1_s11_w2.TIF'))\n",
    "full_img = full_img[None, ..., None]  # add batch and channel dimension\n",
    "\n",
    "print(full_img.shape)\n",
    "\n",
    "full_probs = model.predict(full_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(full_img.squeeze())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(full_probs.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ML)",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
